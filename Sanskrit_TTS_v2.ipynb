{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eWzawfmOjqSr"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2\n",
        "!pip install snac torchcodec \"datasets>=3.4.1,<4.0.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799,
          "referenced_widgets": [
            "e373d613e4ff4c7a86c7cfc4f7ae07c7",
            "fe26cf34958742f7b80d8a773eb6422d",
            "878728ea45904d0bbbb4bef0b1fbae96",
            "98f8683565e641368d827d030d3a2d55",
            "c053d4ebd44848299b889096f614458f",
            "c984ecf9fed74a64873fa98da17b1fa6",
            "1bd9f4cf134e48f9a9550cf287e22c77",
            "1460325ec75942b7b3a99a2d5d30c3b5",
            "98e0967e077145bba1b10e7308589228",
            "f33523e8851543f2932652446041c9a5",
            "4ee6e190d3a648c4ae55340b1f869b7a"
          ]
        },
        "id": "vSc93B8EPR8Y",
        "outputId": "81185e5b-0746-4567-a06c-5a50f1add943"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading models... This may take a moment.\n",
            "Loading models on: cuda\n",
            "==((====))==  Unsloth 2026.2.1: Fast Llama patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.563 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e373d613e4ff4c7a86c7cfc4f7ae07c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models loaded successfully!\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://90152de1c160557cf8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://90152de1c160557cf8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from IPython.display import display, Audio\n",
        "import numpy as np\n",
        "\n",
        "# Global model variables\n",
        "model = None\n",
        "tokenizer = None\n",
        "snac_model = None\n",
        "device = None\n",
        "\n",
        "def load_models():\n",
        "    \"\"\"Initialize and load all required models for Sanskrit TTS inference.\"\"\"\n",
        "    global model, tokenizer, snac_model, device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Loading models on: {device}\")\n",
        "\n",
        "    # Load the fine-tuned Sanskrit TTS model\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        \"R910/Sanskrit_TTS_v2\",\n",
        "        max_seq_length=2048,\n",
        "        dtype=None,\n",
        "        load_in_4bit=False,\n",
        "    )\n",
        "\n",
        "    model = model.to(device)\n",
        "    FastLanguageModel.for_inference(model)\n",
        "\n",
        "    # Load SNAC model for audio generation\n",
        "    try:\n",
        "        from snac import SNAC\n",
        "        snac_model = SNAC.from_pretrained(\"hubertsiuzdak/snac_24khz\").eval()\n",
        "    except ImportError:\n",
        "        print(\"Warning: SNAC model import failed. Make sure SNAC is installed.\")\n",
        "\n",
        "    snac_model.to(\"cpu\")\n",
        "\n",
        "    print(\"Models loaded successfully!\")\n",
        "\n",
        "def redistribute_codes(code_list):\n",
        "    \"\"\"Redistribute generated codes into hierarchical layers for audio synthesis.\"\"\"\n",
        "    layer_1 = []\n",
        "    layer_2 = []\n",
        "    layer_3 = []\n",
        "\n",
        "    for i in range((len(code_list)+1)//7):\n",
        "        layer_1.append(code_list[7*i])\n",
        "        layer_2.append(code_list[7*i+1]-4096)\n",
        "        layer_3.append(code_list[7*i+2]-(2*4096))\n",
        "        layer_3.append(code_list[7*i+3]-(3*4096))\n",
        "        layer_2.append(code_list[7*i+4]-(4*4096))\n",
        "        layer_3.append(code_list[7*i+5]-(5*4096))\n",
        "        layer_3.append(code_list[7*i+6]-(6*4096))\n",
        "\n",
        "    codes = [torch.tensor(layer_1).unsqueeze(0),\n",
        "             torch.tensor(layer_2).unsqueeze(0),\n",
        "             torch.tensor(layer_3).unsqueeze(0)]\n",
        "\n",
        "    audio_hat = snac_model.decode(codes)\n",
        "    return audio_hat\n",
        "\n",
        "def sanskrit_tts_inference(sanskrit_text, chosen_voice=\"\"):\n",
        "    \"\"\"\n",
        "    Generate Sanskrit speech from input text using the fine-tuned model.\n",
        "\n",
        "    Args:\n",
        "        sanskrit_text (str): Input Sanskrit text in Devanagari script\n",
        "        chosen_voice (str): Voice selection parameter (optional)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (audio_data, status_message)\n",
        "    \"\"\"\n",
        "    if not sanskrit_text.strip():\n",
        "        return None, \"Please enter some Sanskrit text.\"\n",
        "\n",
        "    try:\n",
        "        prompts = [sanskrit_text]\n",
        "        chosen_voice = 1070\n",
        "\n",
        "        # Prepare prompts with voice selection\n",
        "        prompts_ = [(f\"{chosen_voice}: \" + p) if chosen_voice else p for p in prompts]\n",
        "\n",
        "        # Tokenize input prompts\n",
        "        all_input_ids = []\n",
        "        for prompt in prompts_:\n",
        "            input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "            all_input_ids.append(input_ids)\n",
        "\n",
        "        # Define special tokens\n",
        "        start_token = torch.tensor([[ 128259]], dtype=torch.int64)\n",
        "        end_tokens = torch.tensor([[128009, 128260]], dtype=torch.int64)\n",
        "\n",
        "        # Construct modified input sequences\n",
        "        all_modified_input_ids = []\n",
        "        for input_ids in all_input_ids:\n",
        "            modified_input_ids = torch.cat([start_token, input_ids, end_tokens], dim=1)\n",
        "            all_modified_input_ids.append(modified_input_ids)\n",
        "\n",
        "        # Apply padding and create attention masks\n",
        "        all_padded_tensors = []\n",
        "        all_attention_masks = []\n",
        "        max_length = max([modified_input_ids.shape[1] for modified_input_ids in all_modified_input_ids])\n",
        "\n",
        "        for modified_input_ids in all_modified_input_ids:\n",
        "            padding = max_length - modified_input_ids.shape[1]\n",
        "            padded_tensor = torch.cat([torch.full((1, padding), 128263, dtype=torch.int64), modified_input_ids], dim=1)\n",
        "            attention_mask = torch.cat([torch.zeros((1, padding), dtype=torch.int64), torch.ones((1, modified_input_ids.shape[1]), dtype=torch.int64)], dim=1)\n",
        "            all_padded_tensors.append(padded_tensor)\n",
        "            all_attention_masks.append(attention_mask)\n",
        "\n",
        "        # Batch tensors for inference\n",
        "        all_padded_tensors = torch.cat(all_padded_tensors, dim=0)\n",
        "        all_attention_masks = torch.cat(all_attention_masks, dim=0)\n",
        "\n",
        "        input_ids = all_padded_tensors.to(device)\n",
        "        attention_mask = all_attention_masks.to(device)\n",
        "\n",
        "        # Generate audio codes using the model\n",
        "        generated_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=1200,\n",
        "            do_sample=True,\n",
        "            temperature=0.6,\n",
        "            top_p=0.95,\n",
        "            repetition_penalty=1.1,\n",
        "            num_return_sequences=1,\n",
        "            eos_token_id=128258,\n",
        "            use_cache=True\n",
        "        )\n",
        "\n",
        "        # Post-process generated tokens\n",
        "        token_to_find = 128257\n",
        "        token_to_remove = 128258\n",
        "\n",
        "        token_indices = (generated_ids == token_to_find).nonzero(as_tuple=True)\n",
        "\n",
        "        if len(token_indices[1]) > 0:\n",
        "            last_occurrence_idx = token_indices[1][-1].item()\n",
        "            cropped_tensor = generated_ids[:, last_occurrence_idx+1:]\n",
        "        else:\n",
        "            cropped_tensor = generated_ids\n",
        "\n",
        "        mask = cropped_tensor != token_to_remove\n",
        "\n",
        "        processed_rows = []\n",
        "        for row in cropped_tensor:\n",
        "            masked_row = row[row != token_to_remove]\n",
        "            processed_rows.append(masked_row)\n",
        "\n",
        "        # Convert tokens to audio codes\n",
        "        code_lists = []\n",
        "        for row in processed_rows:\n",
        "            row_length = row.size(0)\n",
        "            new_length = (row_length // 7) * 7\n",
        "            trimmed_row = row[:new_length]\n",
        "            trimmed_row = [t - 128266 for t in trimmed_row]\n",
        "            code_lists.append(trimmed_row)\n",
        "\n",
        "        # Generate audio samples\n",
        "        my_samples = []\n",
        "        for code_list in code_lists:\n",
        "            samples = redistribute_codes(code_list)\n",
        "            my_samples.append(samples)\n",
        "\n",
        "        if len(my_samples) > 0:\n",
        "            audio_sample = my_samples[0].detach().squeeze().to(\"cpu\").numpy()\n",
        "            return (24000, audio_sample), f\"‚úÖ Generated audio for: {sanskrit_text}\"\n",
        "        else:\n",
        "            return None, \"‚ùå Failed to generate audio - no valid codes produced.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, f\"‚ùå Error during inference: {str(e)}\"\n",
        "\n",
        "# Initialize models\n",
        "print(\"Loading models... This may take a moment.\")\n",
        "load_models()\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(title=\"Sanskrit Text-to-Speech\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üïâÔ∏è Sanskrit Text-to-Speech\n",
        "\n",
        "    Enter Sanskrit text in Devanagari script and generate speech using your fine-tuned model.\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            sanskrit_input = gr.Textbox(\n",
        "                label=\"Sanskrit Text\",\n",
        "                placeholder=\"Enter Sanskrit text in Devanagari script...\",\n",
        "                lines=3,\n",
        "                value=\"‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ ‡§¶‡§¶‡§æ‡§§‡§ø ‡§µ‡§ø‡§®‡§Ø‡§Ç\"\n",
        "            )\n",
        "\n",
        "            generate_btn = gr.Button(\"üéµ Generate Speech\", variant=\"primary\")\n",
        "\n",
        "        with gr.Column():\n",
        "            audio_output = gr.Audio(\n",
        "                label=\"Generated Sanskrit Speech\",\n",
        "                type=\"numpy\"\n",
        "            )\n",
        "\n",
        "            status_output = gr.Textbox(\n",
        "                label=\"Status\",\n",
        "                lines=2,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "    # Example inputs for demonstration\n",
        "    gr.Examples(\n",
        "        examples=[\n",
        "            [\"‡§Ø‡§¶‡§æ ‡§Ø‡§¶‡§æ ‡§π‡§ø ‡§ß‡§∞‡•ç‡§Æ‡§∏‡•ç‡§Ø ‡§ó‡•ç‡§≤‡§æ‡§®‡§ø‡§∞‡•ç‡§≠‡§µ‡§§‡§ø ‡§≠‡§æ‡§∞‡§§‡•§\"],\n",
        "            [\"‡§ï‡§∞‡•ç‡§Æ‡§£‡•ç‡§Ø‡•á‡§µ‡§æ‡§ß‡§ø‡§ï‡§æ‡§∞‡§∏‡•ç‡§§‡•á ‡§Æ‡§æ ‡§´‡§≤‡•á‡§∑‡•Å ‡§ï‡§¶‡§æ‡§ö‡§®‡•§\"],\n",
        "            [\"‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ ‡§¶‡§¶‡§æ‡§§‡§ø ‡§µ‡§ø‡§®‡§Ø‡§Ç\"],\n",
        "            [\"‡§§‡§Æ‡§∏‡•ã ‡§Æ‡§æ ‡§ú‡•ç‡§Ø‡•ã‡§§‡§ø‡§∞‡•ç‡§ó‡§Æ‡§Ø‡•§\"],\n",
        "        ],\n",
        "        inputs=[sanskrit_input],\n",
        "        outputs=[audio_output, status_output],\n",
        "        fn=sanskrit_tts_inference,\n",
        "        cache_examples=False\n",
        "    )\n",
        "\n",
        "    # Connect interface components\n",
        "    generate_btn.click(\n",
        "        fn=sanskrit_tts_inference,\n",
        "        inputs=[sanskrit_input],\n",
        "        outputs=[audio_output, status_output]\n",
        "    )\n",
        "\n",
        "# Launch the application\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(\n",
        "        share=True,\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=7863,\n",
        "        show_error=True\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h5yubedPkVJd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e373d613e4ff4c7a86c7cfc4f7ae07c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe26cf34958742f7b80d8a773eb6422d",
              "IPY_MODEL_878728ea45904d0bbbb4bef0b1fbae96",
              "IPY_MODEL_98f8683565e641368d827d030d3a2d55"
            ],
            "layout": "IPY_MODEL_c053d4ebd44848299b889096f614458f"
          }
        },
        "fe26cf34958742f7b80d8a773eb6422d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c984ecf9fed74a64873fa98da17b1fa6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1bd9f4cf134e48f9a9550cf287e22c77",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "878728ea45904d0bbbb4bef0b1fbae96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1460325ec75942b7b3a99a2d5d30c3b5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98e0967e077145bba1b10e7308589228",
            "value": 2
          }
        },
        "98f8683565e641368d827d030d3a2d55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f33523e8851543f2932652446041c9a5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4ee6e190d3a648c4ae55340b1f869b7a",
            "value": "‚Äá2/2‚Äá[00:23&lt;00:00,‚Äá10.72s/it]"
          }
        },
        "c053d4ebd44848299b889096f614458f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c984ecf9fed74a64873fa98da17b1fa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bd9f4cf134e48f9a9550cf287e22c77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1460325ec75942b7b3a99a2d5d30c3b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98e0967e077145bba1b10e7308589228": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f33523e8851543f2932652446041c9a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ee6e190d3a648c4ae55340b1f869b7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}